# Abstract

Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting.

# Introduction

[IMAGE: intro.png - Contrastive examples provide positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid.]

In recent years, large language models like GPT, Llama, and PaLM series have made significant progress in natural language processing [bommasani2021opportunities; brown2020gpt3; touvron2023llama], enabling them to generate coherent and contextually relevant responses. However, despite their impressive capabilities, these models can still struggle to align with our intent, particularly when it comes to generating content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. As a result, there has been a growing interest in developing techniques that can help us better steer the output of LLMs towards our desired goals. In this work, we propose an approach that leverages contrastive examples to better describe our intent, which can significantly improve the performance of LLMs in generating desirable responses.

In light of the challenges faced by current LLMs in aligning with user intent, it is crucial to develop novel techniques that can effectively guide these models towards generating more desirable responses. Prior research has demonstrated the benefits of few-shot learning [brown2020gpt3], fine-tuning with smaller models [gao2020making], selective annotation frameworks [su2022selective], and visual language modeling [alayrac2022flamingo] for enhancing LLM performance. However, these approaches do not explicitly address the challenge of guiding LLMs to generate content that adheres to specific preferences, styles, or tones. Additionally, although contrastive learning techniques have shown promise in areas such as image representation [radford2021clip], dialogue response ranking [gao2020dialoguerpt], and self-supervised learning [meng2021coco], their application to content generation in LLMs remains underexplored. Furthermore, while recent work on prompt optimization [honovich2022instruction; zhou2022large; sun2023autohint] has highlighted the importance of effective prompts in steering LLMs, there remains a need for more robust methods that can better capture user intent through diverse and contrastive examples. In this paper, we propose a novel approach that addresses these gaps by leveraging contrastive examples, including positive and negative instances, to more accurately define user intent and guide LLMs in generating responses that are better aligned with desired outcomes. By incorporating this contrastive reasoning step, our method aims to overcome the limitations of existing techniques and substantially enhance the performance of LLMs in generating preferable content.

Our proposed approach involves providing the LLM with both positive and negative examples to better understand our intent. The positive examples showcase the desired outcomes, while the negative examples highlight what characteristics the LLM should avoid. By analyzing both types of examples before generating an answer, the model can reason about our intent and make a more informed decision about what to generate. It may be challenging for LLMs to understand negative instruction, but we observed that using negative examples helps aligning our preference with LLMs. Moreover, the negative examples can be retrieved from labeled data, written by a human or generated by the model itself, making it a flexible and scalable technique. Our experiments show that this approach can significantly improve the performance of LLMs in generating desirable responses, making them more useful for a wide range of natural language processing applications.

In summary, our proposed approach of using contrastive examples to better describe our intent significantly improves the performance of LLMs in generating desirable responses. This approach can help address the challenge of aligning LLMs with our intended goals and make them more useful for a wide range of natural language processing applications. The key contributions of this work are:

i) Providing a novel approach that leverages contrastive examples to improve the performance of LLMs in generating desirable responses.

ii) Demonstrating the effectiveness of this approach on both synthesized and real-world datasets, including StackExchange and Reddit.

iii) Highlighting the potential of previously discarded negative examples to make LLMs more useful for a wide range of applications by better aligning them with our intended goals.

# Related Work

Recent advancements in natural language processing have led to the development of large language models (LLMs) that are capable of few-shot learning, where they can learn new tasks with only a small number of annotated examples. Significant works in this area include Brown et al. (2020), who demonstrated the impressive few-shot performance of GPT-3, Gao et al. (2020), who proposed LM-BFF for fine-tuning smaller language models, Su et al. (2022), who presented a selective annotation framework, and Alayrac et al. (2022), who introduced Flamingo, a family of visual language models. These works highlight the importance of few-shot learning in LLMs and the need for efficient annotation and prompt generation techniques.

The use of contrastive learning for LLMs has also gained attention, with Radford et al. (2021) proposing Contrastive Language-Image Pre-training (CLIP) for learning image representations, Gao et al. (2020) leveraging social media feedback data for training a dialogue response ranking model, and Meng et al. (2021) presenting COCO-LM, a self-supervised learning framework that pretrains LLMs by correcting and contrasting corrupted text sequences. These works emphasize the importance of providing diverse and contrastive examples to LLMs and suggest that contrastive learning is a promising direction for LLM research.

Additionally, recent works have focused on improving the performance of LLMs by optimizing the prompts used to steer them towards a desired task or outcome. Honovich et al. (2022) introduce the instruction induction challenge, Zhou et al. (2022) propose Automatic Prompt Engineer (APE) for instruction generation and selection, and Sun et al. (2023) present AutoHint, a framework for automatic prompt engineering and optimization. To apply these approaches for preference alignment, however, is challenging when the characteristics are hard to describe in instruction or measured by automated metrics.

Our work builds upon these previous efforts by proposing an approach that uses contrastive examples to better describe our intent, which we tested on both synthesized and real-world datasets. This approach significantly improves performance compared to standard few-shot prompting, emphasizing the potential for LLMs to become more human-like in their ability to generate natural language instructions.

# Contrastive In-Context Learning

[IMAGE: prompt.png - Contrastive in-context learning prompting utilize contrastive few-shot examples, which consists of a "positive" example and a "negative" example for each demonstration input, and a prompt to elicit analysis of the characteristics of the positive/negative examples, before generating an output for the new input. This strategy can be applied to both conversational and non-conversational LLMs. For non-conversational LLMs, the labels and instructions are provided as system messages (the gear symbols).]

Our method comprises two main components: (1) obtaining paired positive and negative examples and (2) forming the prompt.

## Obtaining Paired Contrastive Examples

There are several ways to obtain contrastive examples.

#### Using Labeled Feedback

In some tasks, there exist multiple natural outputs for a single input, and feedback for these outputs is available. For example, Reddit and StackExchange posts typically receive multiple responses, and users provide feedback through upvotes and downvotes. Similarly, in business applications like email and copywriting, multiple versions might be generated given the same constraints, and feedback (e.g., click-through rate) can be obtained through A/B testing or other experiments. For these tasks, we can use the response with the highest feedback as the "positive" example and the one with the lowest feedback as the "negative" example. It is important to note that a "negative" example here does not necessarily mean it is incorrect or unacceptable, but rather less preferred given the specific audience and scenario. It is also important to note, that the highest versus lowest votes is an indication of popularity which may or may not represent personal preference, but we are using this popularity (or lack thereof) signal as a surrogate for general preference of a particular response over others, ignoring confounding factors such as time of response etc.

#### Using LLM-Generated Responses

In cases where labeled feedback is unavailable or negative examples do not capture the characteristics we want LLMs to avoid, we propose a second method. We let the target LLM generate a response and use it as the negative example. LLMs generated responses often appears mechanical, lacking emotion, and impersonal. Pairing this with a highly preferred labeled response as the positive example guides the LLM to generate a response more aligned with the positive example.

#### Using Automated Evaluator

For some tasks, it is possible to define automated evaluation rules or apply classifiers to measure the quality. The evaluation score can be used to select "positive" and "negative" examples. We demonstrate this with a words-constrained generation task, with accuracy determined by whether all given words were included (scored as 1) or not (scored as 0) in the generated sentences.

## Forming the Prompt

Once we have obtained paired positive and negative examples, we consider two prompting strategies:

#### Contrastive Examples as Few-Shot Examples

In this strategy, we provide the contrastive example pairs as few-shot examples. Labels are included in the prompt, such as "preferred answer" for the positive example and "less preferred answer" for the negative example, to indicate the role of each part in the prompt. The LLM is then asked to generate a "preferred answer" for the new input.

#### Reasoning and Analysis

In addition to the first strategy, we ask the LLM to analyze the reasons for preference and the characteristics of the examples before generating a "preferred answer" for the new input. This reasoning step is inspired by the Chain-of-Thought prompting [wei2022chain] and is designed to summarize the characteristics from the contrastive examples, allowing the LLM to automatically generate instructions for itself. Coupled with contrastive examples, this guidance helps the LLM to better align with the preferred intent.

# Experiments

In this section, we describe the experiments conducted to evaluate the effectiveness of our proposed approach using contrastive examples. We focus on text generation tasks where user preferences play a significant role, rather than tasks with objective right or wrong answers. We first outline the datasets used in our experiments, which include both real-world and synthetic datasets.

## Datasets

We consider the following datasets to investigate the impact of user preferences on text generation tasks:

### Human Preference Datasets

User preference is expressed via different forms of feedback publicly available at a large scale on many platforms.

#### StackExchange

People post questions on StackExchange, and other users provide answers, with upvotes and downvotes used to rate the responses. Our focus is on subjective preferences, so we created a dataset using data from cooking.stackexchange.com, which includes how-to type cooking-related questions. Cooking often does not have right or wrong answers, and different people have different preferences or ways to make a dish they consider good.

#### Reddit

Users post questions or topics on Reddit to trigger discussions, and other users can comment (comments can have nested comments). In our study, we only considered the comments of the posts as they share a similar context. Redditors can provide upvotes or downvotes as feedback for each post and comment. We experimented a subreddit called 'NoStupidQuestions'. 'NoStupidQuestions' encourages people to ask any question without fear of being judged by conventional social norms. The subjects cover a wide range of topics, including objective subjects and social or personal experiences. These characteristics make this subreddit ideal for our study, as personal (or group collective) preference is an essential factor, rather than objective right or wrong.

For both datasets, We filtered the posts to include those with multiple answers (to have answers with varying "quality" or level of preference). For the remaining posts, we used the top-rated answer as the "positive" example, and bottom-rated answer as the "negative" example. We randomly selected 500 samples for evaluation.

### Synthetic Stylistic Datasets

Human preferences play a crucial role in the aforementioned real datasets. However, it is challenging to interpret and analyze the preferences, as it is often difficult even for humans to articulate what exactly makes an answer highly voted. Therefore, we also considered a few simplified synthetic datasets. Using the cooking.stackexchange.com dataset, we leveraged ChatGPT to generate the following datasets: i) Funny vs. Serious, ii) Concise vs. Detailed, and iii) British vs. American

We then used the generated responses as contrastive few-shot examples. It is essential to note that by "positive" and "negative", we do not mean right or wrong, and the choice of "positive" is arbitrary in this case for synthesized datasets. The synthetic datasets aim to illustrate the ability of the proposed method to guide LLMs to generate output in a given direction.

### Constrained Generation Dataset

The datasets mentioned above focus on controlling implicit stylistic aspects of text generation, whether matching a specific attribute such as level of conciseness (as in the synthetic datasets) or an overall holistic style (as in the human perference datasets). In addition to these datasets, we further experiment with a dataset focused on the control of lexical content, constrained generation. This also allows us to test the ability of our method to generalize to tasks with explicit constraints.

Following Zhou et al. (2023), the language model is prompted to craft sentences using specific seed words as constraints. We randomly selected 500 questions from the StackExchange dataset and randomly chose 5 words from each question to serve as lexical constraints. We use the wrong results generated by the LLM as the negative examples.

The model's performance is measured by the success rate, defined as the percentage of sentences generated that contain all 5 given seed words. Success requires an exact uncased match between the generated sentence and constraint words.

## Prompt Settings

In our experiments, we consider two types of large language models (LLMs): non-conversational LLMs, such as GPT-3, and conversational LLMs, including ChatGPT (GPT-3.5-turbo) and GPT-4. We evaluate these LLMs under two different settings: zero-shot and few-shot.

#### Zero-Shot

For non-conversational LLMs, we use a prompt consisting of the post with the prefix "Question:" followed by the second line "Answer:". For conversational LLMs, we use a system prompt stating, "You are a good StackExchange/Reddit user," followed by the user prompt containing the post. In both cases, we input the post title. We exclude the post body, as it often includes edits after the original poster has read the replies, potentially leaking preference information.

#### Few-Shot

For each query, we randomly select k labeled examples as few-shot examples. For non-conversational LLMs, we include the obtained few-shot examples in the prompt, using the "Question:" and "Answer:" prefixes. For conversational LLMs, we use the few-shot examples to create a conversation history between the user (post) and the bot (top-rated reply). We experimented with k=1 to k=4 and stops when performance does not increase significantly as k increases.

### Contrastive Examples Based Prompts

For the few-shot setting, we compare the standard approach, which only includes positive examples, with three settings involving contrastive examples:

#### Contrastive Examples Only

For each few-shot example, we provide a positive and a negative example. For non-conversational LLMs, we use the prefixes "top-rated" and "low-rated" to indicate positive and negative examples, respectively. For conversational LLMs, we use system messages ("provide a top-rated answer" and "provide a low-rated answer") to inform the LLMs of the preference.

#### Contrastive Instruction Only

Given the contrastive examples (with labels defined in method 1), we ask LLMs to automatically generate an instruction based on the preference revealed by the positive and negative examples. A sample prompt could be, "Summarize the characteristics of the preferred and not preferred answer." We then use the generated analysis, followed by "generate a top-rated answer for input," as the instruction for the test input. The contrastive examples are only used to generate the analysis and are not included in the final prompt.

#### Contrastive Examples + Instruction

This method combines the previous two approaches. We first provide the contrastive examples, followed by the instruction asking LLMs to perform an analysis. Then, the LLM generates a new response. When generating the final answer, both the contrastive examples and the generated analysis are available.

These three settings involving contrastive examples allow us to conduct an ablation study to examine the contributions of specific contrastive examples and more general instructions.

## Evaluation Method

In order to evaluate the performance of our approach, we consider two distinct methods: reference-based and reference-free evaluation.

#### Reference-Based Evaluation

The reference-based evaluation method measures the similarity between the generated responses and the top-rated reference answer. We employ two metrics, **BERT Score** and **Emb. Similarity**. The latter is the cosine similarity of the sentence embeddings obtained using the Sentence-BERT model.

#### Reference-Free Evaluation

An answer that deviates from the reference could still be preferred by many readers. Therefore, we also consider two reference-free evaluation method. i) We leverage **DialogRPT**, a pre-trained dialog response ranking model. This model is trained on Reddit data and shows a high correlation with human upvotes, so we apply it to the Reddit dataset only. ii) In addition, we employ LLM as an evaluator following Liu et al. (2023). We compute a **GPT Score** by prompting GPT4 to score the generated results using the positive and negative examples as in-context few shot examples. The LLM generated score and human evaluation labels exhibit positive correlation (StackExchange: 0.65, Reddit: 0.58).

## Results

We evaluated the performance of the baselines and contrastive in-context prompting strategies on the synthetic and two real datasets. The "contrastive-combined" approach (which combines the contrastive examples and the derived instruction) achieved the best performance for most cases. Using similar number of tokens in prompt, this contrastive in-context learning approach performs significantly better than standard few-shot approach. For Reddit dataset, we observe the most obvious improvement with ChatGPT model. Standard two-shot prompt only performs better than zero-shot approach for about 64% test cases, while the "contrastive-combined" approach wins 76% test cases. For StackExchange, standard few-shot approaches do not show obvious improvement compared to zero-shot, while "contrastive-combined" improve BERT Score by approximately 0.01. For synthetic datasets, the improvement of is more obvious, with BERT Score increased by 0.02 to 0.03, and much higher than the improvement achieved with standard few-shot prompts. As an ablation study, we also evaluated the results for "contrastive - examples only" and "contrastive - instruction only". Both approaches improve the performance compared to zero shot, and the "contrastive - examples only" performs better than "contrastive - instruction only" for many cases. Combining them together further improves the results, as discussed above.

#### The Impact of Contrastive Examples

As mentioned above, using contrastive in-context learning by introducing negative examples improve the performance compared to standard few-shot approaches. We futher investigate different ways to obtain negative examples. The first method, "human", uses human-written answers, where low-rated replies serve as "negative" examples. The second method, "generated", uses zero-shot generated replies as "negative" examples. Interestingly, we observed that the second method performs on par with, and sometimes even better than, the first method. This finding demonstrates that the proposed contrastive method is not limited to cases where developers have to provide human-written pairs of positive and negative examples. Instead, it requires the same inputs as standard few-shot settings, which only need a few positive examples since we can generate the negative examples.

This observation also supports our assumption that negative examples obtained from human-written data may not capture all characteristics we want LLMs to avoid. For instance, in the Reddit dataset, a human-written reply may receive downvotes because the content is rude, disrespectful, or violates some rules of the subreddit, such as self-promotion. However, recent LLMs like ChatGPT are generally trained to follow social norms and sometimes even apologize too frequently. As a result, it is highly unlikely for these LLMs to generate offensive language. Providing human-written negative examples, therefore, may not offer much helpful signal in such cases. In contrast, providing generated responses as negative examples can supply the signals of certain characteristics we want the LLM to avoid. For example, although LLM-generated responses are typically fluent and relevant to the question, they often lack emotion, details, examples, or elements that trigger readers' personal feelings. These characteristics, however, are essential factors that make users prefer certain answers. By providing zero-shot generated responses, we can guide LLMs to move away from the machine-generated style and toward a more human-preferred style.

#### Distilling Contrastive Examples as an Instruction

Although the primary focus of this work is not to automatically generate instructions for LLMs, we were interested in understanding what instructions LLMs could derive from the contrastive examples. Therefore, we asked the LLMs to summarize the characteristics of the provided "positive" and "negative" examples. We then used this analysis in the prompt, instead of the actual contrastive examples, to offer the LLMs insight into user preferences. This compress the contrastive examples into a shorter instruction, and reduce prompt length and cost.

Our experimental results showed that, with this analysis, LLMs performed better than in the zero-shot setting. However, the improvement was not significantly better than the standard few-shot setting. This could be partially explained by the fact that the preference was not consistent across the examples chosen based on upvotes. There is room for enhancement in the way the preference is summarized through the use of automated prompt generation approaches. By refining the way LLMs derive instructions from contrastive examples, we can potentially achieve more significant improvements in performance compared to standard few-shot prompting.

#### Combining the Complementary Parts

We observed that when combining the contrastive examples and the instruction (analysis of the characteristics of "positive" and "negative" examples automatically generated based on the contrastive examples) together, an even better performance was achieved. This improved performance can be attributed to the fact that the analysis and the contrastive examples provide complementary information to guide the LLM about user preferences.

The generated instruction exhibits better generalization but may lack the necessary clarity and specificity. This is because the instructions can be vague or provide only general descriptions, which can be challenging for LLMs to interpret. In contrast, the actual contrastive examples offer more detailed information by providing demonstrations of the desired and undesired characteristics.

In summary, our approach of using contrastive examples along with an analysis of the characteristics of positive and negative examples has proven to be a valuable method for aligning LLMs with user preference. This approach not only enhances performance but also provides LLMs with a better understanding of the user's preferences, resulting in more accurate and satisfactory responses.

#### Prompt Token Efficiency

In real-world and industrial applications, the number of prompt tokens plays a crucial role as it is directly related to latency and monetary cost. Ideally, a good prompt should be short yet lead to high-quality output. Therefore, we measure the token efficiency to understand the effectiveness of our approach. The standard few-shot strategies do exhibit an increase in performance as we increase the number of examples. However, the performance, as measured by BERT score, is still lower than the contrastive in-context learning strategies using a similar number of tokens. Furthermore, the contrastive instruction strategy, which summarizes the contrastive examples as an instruction, utilizes fewer tokens but achieves better performance than few-shot examples. This indicates that, given the same budget of prompt tokens, contrastive in-context learning strategies outperform the standard few-shot approaches, thereby demonstrating higher efficiency.

# Conclusion

This research introduced an approach to enhance the alignment of large language models (LLMs) with user preference using contrastive examples. By integrating positive examples showcasing desired outputs and negative ones emphasizing unwanted LLM traits, we tested our methodology on both synthesized and real-world datasets, including StackExchange and Reddit.

The results confirmed the superiority of contrastive examples over the standard few-shot prompting, particularly in terms of performance and prompt token efficiency. Notably, negative examples generated from zero-shot outputs were as effective as those from human-written data. Future endeavors might delve into refining LLM instructions based on these examples and innovating automatic prompt generation techniques.
