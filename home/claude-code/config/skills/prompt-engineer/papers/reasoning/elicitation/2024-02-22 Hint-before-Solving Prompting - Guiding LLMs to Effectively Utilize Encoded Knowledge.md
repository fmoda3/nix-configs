# Abstract

Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We make our code and dataset publicly available at <https://github.com/jinlanfu/HSP>.

# Introduction

Benefiting from extensive training corpora and computational resources, Large Language Models (LLMs) have reached state-of-the-art performance in numerous Natural Language Processing (NLP) tasks [touvron:llama1; openai:gpt4; touvron:llama2; xin:llm_survey; mistral2023mixtral]. However, LLMs still face challenges in complex reasoning tasks, such as mathematical reasoning [lu:mathreason; wizardmath:luo; mathprompter:shima] and commonsense reasoning [common:bhargavi; commonnlp:maarten]. Although possessing a wealth of knowledge, LLMs always fail to accurately apply encoded knowledge to generate coherent and strongly logical reasoning chains when addressing reasoning tasks.

The output comparison of Llama-2-Chat-70B solving a math problem (calculus) with and without a hint. Red text indicates erroneous information; green text indicates correct reasoning. Findings: (1) having a hint can help the LLM understand the problem. (2) The LLM possesses knowledge of calculus, and with a hint, it can accurately apply this knowledge.

To improve the performance of LLMs on complex reasoning tasks, existing works have made several attempts. These previous works include fine-tuning on complete training datasets [wizardmath:luo; metamath:yu; mammoth:yue] , training-free methods based on prompt engineering [zhou:ltm; wang:psp; fu:complexitycot; faithfulcot:lyu; ZhaoLJQB23], or enhancing by retrieving knowledge from external knowledge bases [yao:react; reretrival:he; leanDojo:yang]. Supervised fine-tuning methods are resource-intensive; current prompt engineering seldom attempt to improve LLMs' ability to use accurate knowledge; retrieval augmentation methods are limited to specific tasks. For example, mathematical reasoning that includes many special symbols is difficult to access relevant knowledge through keyword or semantic retrieval.

To mitigate these problems, in this work, we explore how LLMs can effectively utilize their encoded knowledge to enhance their reasoning logic and accuracy. We found that providing LLMs with hints effectively guides their use of encoded knowledge for problem-solving. Fig. [1](#fig:hint_example) illustrates this by comparing Llama2-70B's outputs on a _calculus problem_ with and without hints. The LLM cannot utilize _calculus knowledge_ to solve the problem without any hints, as shown in Fig.[1](#fig:hint_example)-(a). However, when given a hint (as shown in Fig.[1](#fig:hint_example)-(b)): "_\... The second derivative is written `latex $f''(x)$ `._" the LLM can accurately apply its "_calculus knowledge_" to generate a correct and logical solution with intermediate reasoning. The reason can be attributed to that the hint suggested that "`latex $f''(x)$ ` denotes the second derivative", which helped the LLM to better understand the target of the problem. Moreover, we conducted quantitative analysis on six reasoning datasets by introducing hints generated by GPT-4. The experimental results are shown in Fig. [2](#fig:examp_res). We can find that giving high-quality hints can effectively improve reasoning performance.

Results for Llama-2-Chat-70B (under CoT prompting) with or without introducing high-quality hints across six reasoning datasets. Findings: introducing hints lead to significant improvements, with an average relative increase of 9.7%.

However, it is challenging to provide high-quality hints for every sample. To address this problem, we propose the Hint-before-Solving (HSP) prompting method, which allows LLMs to generate hints on their own before solving a problem. The hints may include knowledge necessary for solving the problem (e.g., the hint shown in Fig. [1](#fig:hint_example)-(b)), analyzing the question, and providing essential ideas for the solution. Our explorations of Hint-before-Solving (HSP) Prompting in this paper are driven by following research questions:

Q1: _Can HSP guiding LLMs to autonomously generate helpful hints be effective?_ To answer this question, we incorporated HSP into four well-performing prompting methods to investigate how HSP performs (EXP-I). Furthermore, we examined the effectiveness of the HSP variant, HSP2, which provides hints and solutions in two stages (EXP-II). And explore the upper bound of LLMs under the HSP2 framework (EXP-III). (Sec. [4.1](#sec:can_work))

Q2: _Does HSP still work when dealing with tasks that are challenging for LLMs?_ In other words, if a task is difficult for LLMs, can they still provide helpful hints? To answer this question, we evaluated the challenging MATH dataset (EXP-IV). Furthermore, we explore how LLMs perform under the self-consistency setting (EXP-V). (Sec. [4.2](#sec:hard_task))

Q3: _How do LLMs perform if they are supervised fine-tuned on a large-scale HSP prompting dataset?_ To answer this question, we constructed the HSPMATH dataset based on GSM8K and conducted supervised fine-tuning on Llemma-7B and Llama-2-13B. The experimental results show that we achieved a performance of 61.7 on Llemma-7B, surpassing GPT3.5. (EXP-VI, Sec. [4.3](#sec:sft))

The main contributions of this work are summarized as below:

\(1\) We discovered that providing hints allows LLMs to use their encoded knowledge accurately and effectively. For quantitative analysis, with GPT-4 generated hints, Llama-2-Chat-70B's accuracy increased by nearly 10% across six datasets.

\(2\) We propose the HSP prompting method, allowing LLMs to automatically generate useful hints. We conducted extensive experiments and analyses on applying HSP to four popular prompting methods to verify HSP's effectiveness.

\(3\) We collected 75,000 samples enhanced with hints, namely HSPMATH (to be released), and fine-tuned Llemma-7B to achieve 64.3 accuracy, surpassing GPT-3.5 (57.1) and WizardMath-13B (63.9).

Examples of input and output before (four examples at the top) and after (four examples at the bottom) applying HSP to standard Least-to-Most, Plan-and-Solve, and CoT promptings. The red text in the textbox indicates hints. We find that hints from LLMs, including problem-solving ideas close to the correct answer (e.g., geographical distributions of both species), guide LLMs to use accurate knowledge for correct and logical reasoning.

# Hint-before-Solving Prompting

The prominent Chain-of-Thought (CoT) [wei2022chain] prompting method has inspired various prompting techniques to improve the LLMs' performance. Such as Least-to-Most [zhou2022least], tree-of-thought [tree:yao], graph-oc-thought [graph:besta], plan-and-solve prompting [wang:psp]. In this work, we aim to design a new prompting method that allows LLMs to better utilize their encoded knowledge, namely Hint-before-Solving Prompting (HSP). HSP enables LLMs to explicitly generate hints for solving problems. The hints can be knowledge or key ideas for solving the problem or analyzing the question, etc., and developing an accurate and logical intermediate reasoning process before predicting the final answer.

HSP can be used in conjunction with some of the existing natural language forms of prompting methods (e.g., CoT). Fig. [3](#fig:framework) shows examples of HSP integrated with four existing prompting methods, namely standard prompting, Least-to-Most prompting [zhou2022least], Plan-and-Solve prompting [wang2023plan], and Chain-of-Thought prompting [wei2022chain]. We can observe that the current hints provide LLMs with perspectives for thought (e.g., _consider the geographical distribution of black-tailed jackrabbits \..._), enhancing the effectiveness of prompting methods with the introduction of HSP.

# Experiment

Number G8K ASDiv MArith AQUA MATH SQA Date

---

Samples 1,319 2,097 596 254 5,000 2,290 359
Examples 8 8 8 8 4 6 10

: The number of test samples and prompting examples across seven datasets.

## Large Language Model

To verify the performance of our proposed method, we consider Mixtral-8x7B-Instruct-v0.1 (_Mix-56B_) [mistral2023mixtral] and Llama-2-Chat [touvron2023llama] family models, where Llama-2-Chat-7B (_Lm2-7B_), Llama-2-Chat-13B (_Lm2-13B_), Llama-2-Chat-70B (_Lm2-70B_) were studied. Note, the italicized text in parentheses represents the abbreviated names of the models.

## Datasets

We evaluated the effectiveness of HSP across multiple datasets for mathematical and common sense reasoning tasks. Tab. [1](#tab:statistics) shows the number of test samples for these datasets and the number of samples for prompting in a few-shot setting.

### Mathematical Reasoning

We considered five popular mathematical reasoning datasets, namely *GSM8K (G8K)* [cobbe2021training], *MultiArith (MArith)* [MultiArith], *AQuA* [aqua], *ASDiv* [asdiv], and *MATH* [hendrycks2021measuring].

#### Commonsense Reasoning

Two common sense reasoning datasets were also taken into account, which are *StrategyQA (SQA)* [StrategyQA] and *Date Understanding (Date)* [date].

## Baselines

The baseline Prompting methods considered in this work are listed below:

\(1\) *Standard Prompting (SD)* [brown2020language] generates the answer for the given question without intermediate steps. (2) *Chain-of-Thought Prompting (CoT)* [wei2022chain] generate step-by-step solutions to a given problem. (3) *Least-to-Most Prompting (LtM)* [zhou2022least] involves decomposing a complex problem into simple subproblems. (4) *Plan-and-Solve Prompting (PS)* [wang2023plan] aims to handle the multi-step reasoning task by planning and solving each plan target.

To validate the effectiveness of the our HSP, we reimplemented some previous prompting methods. _To ensure a fair comparison, we did not deliberately reproduce results reported in previous papers but rather aimed to maintain consistency in the experimental setup. For different prompting methods, we kept using the same set of demonstration samples and modified their format according to the prompting method._ To demonstrate the usability of the results reimplemented in our work, we conducted a performance survey on existing baseline prompting with LLMs of comparable strength to those studied in this paper, with the results presented in Appendix [11](#sec:baseline_result).

## Experimental Settings

#### Demonstration examples

Under any prompting method, one dataset is used with the number of demonstration examples in all the experiments discussed in this work. Specifically, as shown in Tab. [1](#tab:statistics), there are 8 demonstration examples each of GSM8K, ASDiv, MArith, and AQUA, 6 examples for StrategyQA, 10 examples for Date, 4 examples for MATH.

#### Hyperparameters of Greedy Decoding

We use the vllm library [^3] for few-shot evaluation. For greedy decoding, the hyperparameters are set as: top_p=1, max_tokens=500, temperature=0, and the number of reasoning path n=1. For self-consistency, the number of reasoning path n is set to 4, 16, 32, 64, 128, and temperature = 0.4. Other hyperparameters are set the same as the greedy decoding. All inference experiments are based on four A100 GPUs.

# Experiments and Results

+-----------------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+-------------+------------+------------+------------+
| Method | HSP | G8K | ASDiv | MArith | AQUA | SQA | Date | Avg | Improvement | | | |
+:========+========================:+:=======================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:===========:+:==========:+:==========:+:==========:+
| Lm2-7B | SD | `latex $\times$ ` | [5.8] | 43.7 | [7.4] | 19.7 | 62.0 | 33.1 | 28.6 | [IMAGE: image] |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | 5.5 | [44.8] | 6.5 | [21.3] | [63.8] | [39.8] | [30.3] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | LtM | `latex $\times$ ` | 15.5 | 49.5 | 21.8 | [26.0] | 63.9 | [49.3] | [37.7] | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [16.0] | [50.2] | [29.2] | 23.2 | [65.3] | 42.3 | [37.7] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | PS | `latex $\times$ ` `latex $^\ddag$ ` | [21.8] | 55.8 | [66.6] | [25.6] | 58.1 | [34.8] | 43.8 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` | 21.5 | [56.8] | 60.6 | 25.2 | [60.5] | 33.4 | [43.0] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | CoT | `latex $\times$ ` | 19.7 | 53.6 | 63.4 | [24.4] | 66.3 | 40.1 | 44.6 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [19.9] | [55.8] | [63.8] | [24.4] | [67.5] | [43.2] | [45.8] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | [Rlt Avg] | [0.0] | [1.2] | [0.2] | [-0.4] | [1.7] | [0.3] | [0.5] | |
+---------+-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------------+
| Lm2-13B | SD | `latex $\times$ ` | [8.5] | 48.6 | 10.1 | 19.3 | 65.3 | 40.7 | 32.1 | [IMAGE: image] |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | 8.2 | [49.9] | [11.7] | [21.3] | [68.4] | [55.2] | [35.8] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | LtM | `latex $\times$ ` | 23.8 | 55.8 | 52.7 | [31.1] | 68.8 | [60.4] | [48.8] | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [27.6] | [55.9] | [57.7] | 23.2 | [69.6] | 51.3 | 47.6 | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | PS | `latex $\times$ ` `latex $^\ddag$ ` | [35.1] | [63.0] | [80.7] | [25.6] | 60.9 | 47.6 | 52.2 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` | 32.4 | 62.9 | 74.8 | [25.6] | [66.0] | [50.1] | [52.0] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | CoT | `latex $\times$ ` | 34.5 | 60.5 | 83.2 | [25.6] | 68.0 | [57.7] | 54.9 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [36.5] | [61.2] | [87.1] | [25.6] | [72.1] | [57.7] | [56.7] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | [Rlt Avg] | [0.7] | [0.5] | [1.1] | [-1.5] | [3.3] | [2.0] | [1.0] | |
+---------+-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------------+
| Lm2-70B | SD | `latex $\times$ ` | 12.6 | 60.6 | [26.3] | 24.8 | 72.9 | 54.6 | 42.0 | [IMAGE: image] |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [12.8] | [62.7] | 25.7 | [25.6] | [75.5] | [76.6] | [46.5] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | LtM | `latex $\times$ ` | 40.2 | 68.6 | 72.0 | [39.4] | 75.2 | 71.0 | 61.1 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [41.9] | [69.4] | [76.8] | 38.6 | [77.0] | [77.4] | [63.5] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | PS | `latex $\times$ ` `latex $^\ddag$ ` | [60.0] | [74.1] | [95.8] | [40.2] | [64.7] | 62.4 | 66.2 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` | 55.5 | 72.7 | 93.0 | 36.2 | 58.9 | [63.8] | [63.4] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | CoT | `latex $\times$ ` | 46.1 | 72.5 | 93.8 | 35.8 | 74.6 | 71.6 | 65.7 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [50.3] | [74.4] | [94.6] | [37.0] | [77.0] | [73.0] | [67.7] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | [Rlt Avg] | [0.4] | [0.9] | [0.6] | [-0.7] | [0.2] | [7.8] | [1.5] | |
+---------+-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------------+
| Mix-56B | SD | `latex $\times$ ` | 19.8 | 64.3 | [44.6] | 22.0 | [72.1] | 45.4 | 44.7 | [IMAGE: image] |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [20.3] | [65.9] | 38.9 | [30.7] | 71.2 | [61.3] | [48.1] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | LtM | `latex $\times$ ` `latex $^\ddag$ ` | [56.0] | [77.1] | [74.3] | 43.3 | [73.9] | 64.1 | 64.8 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` | [56.0] | 77.0 | 72.8 | [49.2] | 72.4 | [64.3] | [65.3] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | PS | `latex $\times$ ` `latex $^\ddag$ ` | [73.2] | [84.2] | [97.8] | [49.6] | 66.3 | [68.5] | 73.3 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` | 67.1 | 82.3 | 92.3 | 48.4 | [67.6] | 66.6 | [70.7] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | CoT | `latex $\times$ ` | 63.7 | 78.3 | 96.1 | 42.5 | 74.7 | 69.9 | 70.9 | |
| | +-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | | `latex $\checkmark$ ` `latex $^\dag$ ` | [69.8] | [80.1] | [97.0] | [48.4] | [75.1] | [77.4] | [74.6] | |
| +-------------------------+-------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+ |
| | [Rlt Avg] | [0.1] | [0.4] | [-2.9] | [4.8] | [-0.2] | [5.4] | [1.3] | |
+---------+---------------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------------------+

## Q1: Can HSP Work?

In this section, we considered three perspectives to answer whether HSP can enhance LLMs' performance by generating hints containing specific knowledge, pivotal concepts, or analytical insights critical for solving the problem before attempting to solve it. Next, we will illustrate the three perspectives in detail.

### Exp-I: When HSP Meets Existing Prompting Methods

We applied HSP to four existing popular prompting methods to explore how HSP performs in different prompting methods. Our experimental prompting methods include standard prompting (SD), Least to Most prompting (LtM), Plan-and-Solve prompting (PS), and CoT prompting, as introduced in Sec. [3.3](#sec:baselines) The results are shown in Tab. [\[tab:effect_hsp\]](#tab:effect_hsp). The main findings are summarized as below:

\(1\) _HSP is effective in standard and CoT prompting but fails in PS and LtM prompting._ From Tab. [\[tab:effect_hsp\]](#tab:effect_hsp), we observe that the standard and CoT Prompting show significant performance improvements under HSP, while the enhancements from PS and LtM are limited. We try to give reasons below: Hints clarify the prompt or problem by offering key insights or solutions, influencing the logic behind the answers. They are crucial in task planning for both PS and LtM prompting, where introducing hints early can impact their planning process. Conversely, Standard and CoT prompting, focusing solely on the final answer or intermediate reasoning, are compatible with hints.

\(2\) _Larger model sizes tend to show more significant performance improvements._ From Tab. [\[tab:main_result\]](#tab:main_result), we can observe that the average performance improvements for 7B, 13B, 56B, and 70B models across four prompting methods (e.g., CoT and LtM) are 0.5, 1.0, 1.3, and 1.5, respectively. The reason can be that the model capabilities increase as the size increases, and higher capabilities will help achieve higher quality hints for better problem-solving.

\(3\) _The introduction of HSP can steadily enhance the performance of CoT prompting._ We observe that CoT, combined with HSP, shows performance enhancements across all four models and six datasets, while SD, LtM, and PS all experience some scenarios of performance drop. From the line chart in Tab. [\[tab:effect_hsp\]](#tab:effect_hsp), we can observe that LtM and PS exhibit significant fluctuations in average performance gains across each dataset, with numerous settings of negative improvement.

+------------------------------------+----------+----------+----------+----------+----------+----------+----------+-----------------------------------------------------------------------------+------------+------------+
| Method | G8K | ASDiv | MArith | AQUA | SQA | Date | Avg | Improvement | | |
+:===================+:==============+:========:+:========:+:========:+:========:+:========:+:========:+:========:+:=================================================:+:=======================:+:==========:+:==========:+
| Lm2-7B | CoT | 19.7 | 53.6 | 63.4 | 24.4 | 66.3 | 40.1 | 44.6 | [IMAGE: image] | [IMAGE: image] |
| +---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP`latex $^\dag$ ` | 19.9 | **55.8** | **63.8** | 24.4 | **67.5** | **43.2** | **45.8** | | |
| +---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP2`latex $^\ddag$ ` | **22.6** | 55.4 | 62.6 | **25.2** | 66.8 | 40.4 | 45.5 | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| 1-9 \[2\]\*Lm2-13B | CoT | 34.5 | 60.5 | 83.2 | 25.6 | 68.0 | 57.7 | 54.9 | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP`latex $^\dag$ ` | **36.5** | 61.2 | **87.1** | 25.6 | **72.1** | **57.7** | **56.7** | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP2`latex $^\ddag$ ` | **36.5** | **61.9** | 85.1 | **26.0** | 70.7 | 57.0 | 56.2 | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+---------------------------------------------------+---------------------------------------------------+
| Lm2-70B | CoT | 46.1 | 72.5 | 93.8 | 35.8 | 74.6 | 71.6 | 65.7 | [IMAGE: image] | [IMAGE: image] |
| +---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP`latex $^\dag$ ` | 50.3 | **74.4** | **94.6** | 37.0 | **77.0** | 73.0 | 67.7 | | |
| +---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP2`latex $^\ddag$ ` | **54.3** | 73.9 | 93.0 | **37.8** | 71.5 | **76.0** | **67.8** | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| 1-9 \[2\]\*Mix-56B | CoT | 63.7 | 78.3 | 96.1 | 42.5 | 74.7 | 69.9 | 70.9 | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP`latex $^\dag$ ` | 69.3 | 80.1 | **97.0** | **48.4** | **75.1** | 77.4 | **74.6** | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+ | |
| | +HSP2`latex $^\ddag$ ` | **69.8** | **80.3** | 96.8 | 45.7 | 74.3 | **79.1** | 74.3 | | |
+--------------------+---------------+----------+----------+----------+----------+----------+----------+----------+---------------------------------------------------+---------------------------------------------------+

+------------------------------------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| Method | G8K | ASDiv | MArith | AQUA | SQA | Date | Avg |
+:=======================:+:======================================:+:==========================================:+:=========================================:+:==========================================:+:=========================================:+:=========================================:+:==========================================:+:=========================================:+
| ChatGPT | 79.1 | \- | 97.3 | 55.1 | \- | \- | \- |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| 7B | HSP2 | 22.6 | 55.4 | 62.6 | 25.2 | 66.8 | 40.4 | 45.5 |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | HSP2G | **39.0** | **62.5** | **88.9** | **28.7** | **69.5** | **61.0** | **58.3** |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | [Impv] | [16.4] | [7.1] | [26.3] | [3.5] | [2.7] | [20.6] | [12.8] |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| 13B | HSP2 | 36.5 | 61.9 | 85.1 | 26.0 | 70.7 | 57.0 | 56.2 |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | HSP2G | **56.4** | **66.4** | **95.6** | **36.6** | **72.0** | **69.4** | **66.1** |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | [Impv] | [19.9] | [4.5] | [10.5] | [10.6] | [1.3] | [12.4] | [9.9] |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| 70B | HSP2 | 54.3 | 73.9 | 93.0 | 37.8 | 71.5 | 76.0 | 67.8 |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | HSP2G | **68.2** | **79.0** | **98.0** | **43.3** | **76.6** | **87.7** | **75.5** |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | [Impv] | [13.9] | [5.1] | [5.0] | [5.5] | [5.1] | [11.7] | [7.7] |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| 56B | HSP2 | 69.8 | 80.3 | 96.8 | 45.7 | 74.3 | 79.1 | 74.3 |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | HSP2G | **79.5** | **84.1** | **99.2** | **56.3** | **76.5** | **84.7** | **80.1** |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| | [Impv] | [9.7 ] | [3.8 ] | [2.4 ] | [10.6 ] | [2.2 ] | [5.6 ] | [5.7 ] |
+-------------------------+----------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+
| [Avg impv] | [15.0] | [5.1] | [11.1] | [7.6] | [2.8] | [12.6] | [9.0] |
+------------------------------------------------------------------+--------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+

: Experimental results of enhancing HSP2 with hints generated by GPT4. The values in green are the performance gap between HSP2G and HSP2. The blue values are the improvement across the four models. The values in bold represent the best performance.

### EXP-II: Effectiveness of HSP for CoT Prompting

In Exp-I, we found that applying HSP to CoT prompting results in significant and stable performance improvements across six datasets. Based on this, to identify flexible and effective ways to incorporate HSP, we attempted to explore whether a two-stage HSP (HSP2) approach could work in CoT prompting. The two-stage HSP means that LLMs produce outputs twice, first outputting a hint and then a solution. In contrast, HSP has only one output that contains both the hint and the solution. Experimental results on 6 datasets of 4 open source models are shown in Tab. [\[tab:main_result\]](#tab:main_result). The main observations are summarized as below:

\(1\) _The performance of HSP and HSP2 is comparable, despite the different ways of introducing hints._ We can observe that among four LLMs, the largest average performance gap between HSP and HSP2 across six datasets was achieved on the Llama2-13B model with 0.5% (56.7-56.2). This indicates that although the methods of introducing hints differ, the extent of performance improvement brought by both is close.

\(2\) _HSP brings more stable improvements compared to HSP2._ From histograms in Tab. [\[tab:main_result\]](#tab:main_result), HSP shows improvements on nearly every dataset under models of four different sizes. In contrast, HSP2 may lead to performance decreases in certain scenarios, for example, on the MArith dataset, the HSP2 performance decreases with Llama2-7B and Llama2-70B models.

### Exp-III: The Impact of Hint Quality

Introducing HSP can effectively enhance the performance of CoT prompting. But what is the upper bound? Here, we choose to explore on HSP2 because it enables the hints from external sources, a feature not available in the one-stage HSP structure, and HSP2 is comparable in strength to HSP (Sec. [4.1.2](#sec:comp_variants)). Hints generated by GPT-4 will be used as part of the input in the HSP2, denoting as HSP2G. Experimental results are shown in Tab. [2](#tab:gpt4_hint). The performance of ChatGPT is copied from @yin:eot, where the number of examples used to evaluate GSM8K, MultiArith, and AQUA is 8, 8, and 4, respectively. The main findings are summarized as below:

\(1\) _High-quality hints make the open-source model outperforms ChatGPT._ We can observe that with the introduction of high-quality hints, all of the four LLMs with different model sizes and structures consistently showed performance improvement across six datasets. Furthermore, the Mix-56B equipped with HSP2(GPT4) outperformed ChatGPT on the GSM8K, MultiArith, and AQUA datasets.

\(2\) _The introduction of high-quality hints leads to more improvements in lower-capability models._ Tab. [2](#tab:gpt4_hint) shows that the average performance improvements for the Llama2 models sized 7B, 13B, and 70B are 12.8, 9.9, and 7.7, respectively. This indicates that with the support of high-quality hints, HSP2(GPT4)'s performance has improved a lot compared to HSP2. This can be attributed to that the low capability LLMs are hard to generate helpful hints that can assist in providing correct solutions. By providing high-quality hints, it is possible to offer more benefits beyond the capability of lower-ability LLMs. Therefore, there is a relatively large improvement in performance.

## Q2: Can HSP Work on Hard Tasks?

### EXP-IV: Exploring Difficult Tasks

Prompting Lm2-7B Lm2-13B Lm2-70B Mix-56B

---

CoT **4.5** 5.6 11.1 27.0
+HSP 4.4 **5.7** **11.4** **28.6**`latex $^\dag$ `

: Results on MATH dataset. Values in bold denote the best performance, and the value with `latex $^\dag$ ` denotes the performance of HSP significantly outperforms CoT.

+----------------+-----------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Param. | Prompting | Overall | Type | Level |
+:===============+:==========+:=======:+:======================================:+:==========================================:+:==========================================:+:==========================================:+:======================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:==========================================:+:======================================:+
| 4-10 (lr)11-15 | | | AG | CP | GT | IA | NT | PG | PC | L1 | L2 | L3 | L4 | L5 |
+----------------+-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| n=1,t=0 | CoT | 27.0 | 39.01 | 18.99 | 18.58 | 13.4 | 16.85 | 47.07 | 15.57 | 62.47 | 44.41 | 30.59 | 18.62 | 8.08 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | +HSP | 28.6 | 39.09 | 23.21 | 21.09 | 13.84 | 15.93 | 46.27 | 15.2 | 64.3 | 45.64 | 30.33 | 18.29 | 8.91 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | Impv | 1.62 | [0.08] | [**4.22**] | [**2.51**] | [0.44] | [-0.92] | [-0.8] | [-0.37] | [**1.83**] | [**1.23**] | [-0.26] | [-0.33] | [0.83] |
+----------------+-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| n=4,t=0.4 | CoT | 31.9 | 46.67 | 26.58 | 22.55 | 15.39 | 20.56 | 52.47 | 17.95 | 71.17 | 49.33 | 36.6 | 23.39 | 10.8 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | +HSP | 33 | 47.35 | 26.37 | 26.1 | 15.39 | 21.3 | 54.88 | 19.6 | 72.31 | 51.45 | 36.34 | 25.86 | 11.33 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | Impv | 1.1 | [0.68] | [-0.21] | [3.55] | [0] | [0.74] | [**2.41**] | [**1.65**] | [**1.14**] | [**2.12**] | [-0.26] | [**2.47**] | [0.53] |
+----------------+-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| n=16,t=0.4 | CoT | 37.6 | 53.41 | 31.22 | 27.35 | 19.38 | 26.67 | 58.9 | 24.73 | 78.03 | 56.71 | 43.15 | 30.07 | 13.52 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | +HSP | 38.8 | 53.75 | 32.07 | 31.52 | 20.93 | 27.59 | 59.82 | 26.01 | 78.49 | 57.83 | 44.39 | 33.11 | 13.44 |
| +-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+
| | Impv | 1.2 | [0.34] | [0.85] | [**4.17**] | [**1.55**] | [0.92] | [0.92] | [**1.28**] | [0.46] | [**1.12**] | [**1.24**] | [**3.04**] | [-0.08] |
+----------------+-----------+---------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+--------------------------------------------+----------------------------------------+

The tasks we have explored are those that LLMs can handle well. As the difficulty of the task increases, LLMs may not possess sufficient knowledge and capability to address it. This raises a research question: Can LLMs generate helpful hints when they meet the challenge task?

To answer this question, we chose to investigate the MATH dataset [math:hendrycks], which is a dataset that poses challenges for large language models. The results are shown in Tab. [3](#tab:math_eval). We can observe that only the Mix-56B model shows a significant improvement of 1.6 under CoT+HSP prompting, while the Llama-2 family model fails. The reason might be that the Llama-2 family models face significant challenges on the MATH dataset, with their best result being only 11.4 (Lm2-70B), while the Mix-56B model achieves 27.0 under CoT prompting, it is difficult for Llama-2 family model to generate valuable hints.

To find which kind of samples Mix-56B can work, we performed a fine-grained analysis based on the mathematic problem topic and the difficulty, where the dataset provides the topics and the difficulty levels. Furthermore, to explore how self-consistency affects the performance, we evaluate this model using sample paths of n=4 and n=16 and a model temperature of 0.4. The results are shown in Tab. [\[tab:math_fine-grained\]](#tab:math_fine-grained). The main findings can be summarized as: (1) As n increases, under the CoT+HSP setting, the samples for which the LLM sees performance improvements shift from low to high difficulty. (2) As n increases, it is commonly believed that the most challenging GT type experiences the most significant performance improvement, amounting to 4.17. These indicate that by increasing n, HSP enhancement will correctly solve more complex questions.

The relative performance improvement of self-consistency between CoT+HSP and CoT. The numbers of sample paths are 4, 16, 32, and 128, and the model temperature is 0.4.

### EXP-V: The Impact of Self-consistency

In EXP-IV (Sec. [4.2.1](#sec:exp4_hard_task)), we found that self-consistency setting can improve performance of difficult tasks (MATH dataset), even difficult samples. This raises the question of how CoT prompting equipped with HSP performs under a self-consistency setting for the popular tasks. We sample paths with numbers (n) 4, 16, 32, and 128 for the self-consistency study and set the model temperature as 0.4. The relative improvement between CoT+HSP and CoT on six datasets is shown in Fig. [4](#fig:sc_rlt_impv) (Full results can be seen in the Appendix [12](#sec:sc_full_appendix)). The main findings are as below:

\(1\) _As the number of sampling paths increases, the relative improvements brought by applying HSP also increase._ From Fig. [4](#fig:sc_rlt_impv), we can observe that at n=32 or n=128, all three models achieve their best performance. By calculating the Pearson correlation between the number of sampling (n) and relative performance for Lm2-7B, Lm2-13B, and Lm2-70B (excluding n=128), the correlations are 0.67, 0.72, and 0.95, respectively. The reason can be that the larger n leads to more explored hints, making it easier to generate hints beneficial for problem-solving.

\(2\) _Smaller models see the most significant relative performance improvement after applying self-consistency._ This might be because smaller models have lower capabilities, while with guided hints, increasing n makes it easier to correct originally incorrect solutions, thus leading to more substantial performance improvements.

## Q3 (EXP-VI): How does SFT Perform on HSP Format Datasets?

Despite the remarkable success of LLMs, most existing open-source LLMs (e.g., LLaMA-2) still face challenges in solving math problems due to complex reasoning processes. How do LLMs perform when they are supervised fine-tuning (SFT) on the HSP format dataset?

We construct a SFT dastaset with CoT+HSP format. Specifically, we collected hints by GPT4 for the GSM8K training set with 7.5k samples. 75,000 samples that rewrite the original questions from the MetaMATH [metamath:yu], are extracted. And the hints will be utilized to the derived questions. The dataset is named HSPMATH, and the original 7.5k samples will be used as standard samples, which we call HSPMATH-1.s The results with supervised fine-tuning on GSM8K under Llemma-7B and Llama2-13B are shown in Tab. [4](#tab:math_sft). The baselines include: Llama2 [touvron2023llama], RFT [RFT], Llemma [Llemma], WizardMath [WizardMath], WizardLM [xu2023wizardlm], MetaMath [metamath:yu], GPT-3.5 [openai:gpt4], PaLM [palm], Minerva [Minerva], and Chinchilla [Chinchilla] We can observed:

\(1\) _Supervised fine-tuning on datasets with HSP allows LLMs to achieve significant performance improvements._ From Tab. [4](#tab:math_sft), we can observe that in three groups of SFT (e.g., HSP-Llemma vs. Llemma on the HSPMATH1 dataset), the performance dramatically improves with HSP enhancement, which is 5.1, 12.3, and 5.6, respectively. The reason can be that supervised fine-tuning involving hints helps the model better utilize encoded knowledge during the reasoning stage, thereby improving the model's generalization ability.

\(2\) _The result of HSP-Llemma-7B surpassed many popular LLMs, including GPT-3.5 and WizardMath._ By fine-tuning the HSPMATH dataset with 75k CoT+HSP format samples, our HSP-Llemma-7B achieved a competitive performance of 64.3, surpassing closed-source models such as GPT-3.5 (57.1) and PaLM-540B (56.5), and WizardMath-13B (63.9), which was fine-tuned on a large-scale mathematical corpus. It approaches the performance of MetaMath-7B (66.5), fine-tuned on a corpus of 40k samples.

+-------------------+--------+--------+------------+------------+------------+
| Model | Size | ACC | Model | Size | ACC |
+:==================+:======:+:======:+:==========:+:==========:+:==========:+
| 1-3(lr)4-6 | _close source_ | | |
+-------------------+--------+--------+------------+------------+------------+
| 1-3(lr)4-6 Llama2 | 7B | 14.6 | GPT-3.5 | \- | 57.1 |
+-------------------+--------+--------+------------+------------+------------+
| Llama2 | 13B | 28.7 | PaLM | 540B | 56.5 |
+-------------------+--------+--------+------------+------------+------------+
| Llemma | 7B | 36.4 | Minerva | 540B | 58.8 |
+-------------------+--------+--------+------------+------------+------------+
| Llama2 | 34B | 42.2 | Minerva | 62B | 52.4 |
+-------------------+--------+--------+------------+------------+------------+
| RFT | 7B | 50.3 | Chinchilla | 70B | 43.7 |
+-------------------+--------+--------+------------+------------+------------+
| 4-6 Llemma | 34B | 51.5 | _HSPMATH-1 (7.5k samples)_ |
+-------------------+--------+--------+------------+------------+------------+
| 4-6 RFT | 13B | 54.8 | Llemma | 7B | 46.8 |
+-------------------+--------+--------+------------+------------+------------+
| WizardMath | 7B | 54.9 | HSP-Llemma | 7B | 51.9 |
+-------------------+--------+--------+------------+------------+------------+
| WizardLM | 13B | 55.3 | Llama2 | 13B | 42.6 |
+-------------------+--------+--------+------------+------------+------------+
| Llama2 | 70B | 56.8 | HSP-Llama2 | 13B | 54.9 |
+-------------------+--------+--------+------------+------------+------------+
| 4-6 WizardMath | 13B | 63.9 | _HSPMATH (75k samples)_ |
+-------------------+--------+--------+------------+------------+------------+
| 4-6 MetaMath | 7B | 66.5 | Llemma | 7B | 58.7 |
+-------------------+--------+--------+------------+------------+------------+
| MetaMath | 13B | 72.3 | HSP-Llemma | 7B | **64.3** |
+-------------------+--------+--------+------------+------------+------------+

: The results of supervised fine-tuning on GSM8K. The value in bold denotes best SFT result.

# Related Work

Chain-of-thought (CoT) has given a lot of inspiration to many works and has made numerous attempts to explore high performance. These techniques include using programming languages to represent the reasoning process [pal:gao; faithfulcot:lyu], representing the reasoning process with complex structures such as trees or graphs [tree:yao; graph:besta], task decomposition [zhou2022least; decom1] and combining different prompting [verify:liu; JianpengZhou].

For the use of hint enhancement, @php proposed Progressive-Hint Prompting (PHP), which aims to enhance LLMs' effectiveness by introducing hints iterative, where the hint is a numerical value obtained from the previous solution (or base prompt's solution). However, the hints for our HSP come from LLMs themselves, while PHP comes from previous predictions. Moreover, our hints can be one-stage, whereas PHP must be multi-staged.

# Analysis

## Length of Reasoning

Can HSP enhance the model's reasoning capability and effectively reduce the length of the solution generated? To answer this question, we calculated the solution lengths for CoT and CoT+HSP (applying HSP to CoT). For easy understanding, we divided the solution length of CoT+HSP by the solution length of CoT, with the results shown in Fig. [5](#fig:hsp_length_comp), where the red horizontal line indicates that the solution lengths of CoT and CoT+HSP are equal.

Our main observation are summarized as below:

\(1\) _Introducing HSP can effectively reduce the length of the solution._ From Fig. [5](#fig:hsp_length_comp), we can observate that, out of 24 results across four models and six datasets, only 5 instances show CoT+HSP having a longer solution length than CoT.

\(2\) _The effect of reducing the solution length by introducing HSP is most pronounced in mathematical reasoning tasks._

The ratio of solution lengths between CoT and HSP+CoT (HSP applied to CoT prompting). The red line (y=1) indicates that the solution lengths of CoT equals to HSP+CoT.

## Case Study

Guiding the model to generate hints before the solution can effectively improve the model's performance. So, how does guiding LLM to generate hints first affect the generation of the model's solution? We choose to introduce hints under CoT prompting and select case studies on mathematical reasoning and common sense reasoning tasks, as shown in Tab  [\[tab:case_study_part\]](#tab:case_study_part).

#### Case 1

For the question, \"Could a Jujutsu expert hypothetically defeat a Janissary?\". Under CoT prompting, the LLM-generated solution only explained what "Jujutsu expert" and "Janissary" are. However, in CoT+HSP, the generated hint mentioned analyzing the possibility of the Jujutsu expert defeating Janissary from the perspectives of "martial arts skills" and "weapons," thus making a correct solution followed up after introducing the hint.

#### Case 2

The solution from CoT seems reasonable, but when calculating the annual total income of a teacher and coach, it was not multiplied by the hourly wage, leading to a final miscalculation. In contrast, CoT+HSP, within the hint, provided the problem-solving ideas, allowing for the correct answer to be calculated step by step in the solution based on the problem-solving strategy mentioned in the hint.

**Case 1: Commonsense reasoning**

---

[Question]: Could a Jujutsu expert hypothetically defeat a Janissary?
[True Answer]: FALSE
[CoT Response]: Jujutsu is a martial art that focuses on using an opponent's strength against them.Janissaries were elite soldiers in the Ottoman Empire.Thus, a Jujutsu expert could potentially defeat a Janissary. So the answer is yes. [\[WRONG\]]
[CoT+HSP Response]:
Hint: Compare the [martial arts skills of a Jujutsu expert with the combat training of a Janissary, considering the historical context and weaponry.]
Solution: A Jujutsu expert has martial arts skills, but no weapons.A Janissary is an elite Ottoman soldier with advanced combat training and weaponry. Thus, a Janissary would likely defeat a Jujutsu expert. So the answer is no. [\[CORRECT\]]
**Case 2: Mathematical reasoning**
[Question]: Jill gets paid \```latex $20 per hour to teach and \$ `30 to be a cheerleading coach. If she works 50 weeks a year, 35 hours a week as a teacher and 15 hours a week as a coach, what's her annual salary?
  [True Answer]: 57500
  [CoT Response]: Jill works 50 weeks a year. She works 35 hours a week as a teacher, so that's 35 x 50 = \`latex $1750 per year. She works 15 hours a week as a coach, so that's 15 x 50 = \$ `750 per year. Her total annual salary is \`latex $1750 + \$ `750 = \`latex $2500. The answer is \$ `2500. [\[WRONG\]]
  [CoT+HSP Response]:
  Hint: To find out Jill's annual salary, [first calculate how much she makes per week as a teacher and as a coach. Then, multiply this total amount by the number of weeks she works in a year. This will give you Jill's annual salary.]
  Solution: Jill makes \`latex $20 per hour as a teacher, so 35 hours a week is 35 x 20 = 700 per week. As a coach, she makes \$ ```30 per hour, so 15 hours a week is 15 x 30 = 450 per week. 700 + 450 = 1150 per week. 1150 x 50 weeks = 57500 per year. The answer is 57500. [\[CORRECT\]]

# Conclusion

In this work, we present Hint-before-Solving Prompting (HSP), a technique that directs Large Language Models (LLMs) to initially produce hints that assist in problem-solving before generating solutions that incorporate intermediate reasoning steps. This method alleviates the problem that LLMs, despite having vast knowledge, still encounter in effectively utilizing their encoded knowledge to construct precise and rational reasoning paths. Through extensive experimental analysis, we have drawn several main findings: (1) HSP can guide LLMs to generate knowledge or key ideas to problems, thereby helping LLMs to generate more logically coherent reasoning paths to reach the correct answers (Sec. [4.1.1](#sec:exp1_hsp)). (2) For the high-quality hint, the performance improvement of open-source models can reach 12.8, even surpassing ChatGPT (Sec. [4.1.3](#sec:high-quality)). (3) When meets challenging tasks, HSP fails on low-capability open-source LLMs (e.g., Llama2-7B); however, on high-capability open-source LLMs, under the self-consistency setting, HSP improves a lot on the samples with difficult topics or hard levels (Sec. [4.2.1](#sec:exp4_hard_task)). (4) Supervised fine-tuning on the GSM8K training dataset with the CoT+HSP format, our HSP-Llemma-7B (64.3) outperform GPT3.5 (57,1) and WizardMath-13B (63.9) (Sec. [4.3](#sec:sft)).

# Limitation

Here, we summarize some limitations of this paper, as follows: (1) The HSPMATH dataset was expanded by rewriting questions from GSM8K nine times, but our hints were generated based only on the original samples and applied to the nine rewritten samples. The rewritten samples might undergo logical changes, making the introduction of hints less harmonious. There might be a risk of poor performance during supervised fine-tuning. In the future, we will refine this dataset carefully and release a new version. (2) Due to limitations in computational resources, this paper did not conduct supervised fine-tuning on models larger than 13B parameters in the SFT experiments, resulting in an incomplete exploration of HSP-enhanced supervised fine-tuning. We will undertake this exploration in the future.

# Robustness Analysis

Considering the impact that varying sets of examples may have on results, the question arises: Is the HSP framework effective with diverse example sets?

To investigate this, we conducted experiments on the GSM8K (mathematical reasoning) and StrategyQA (common sense reasoning) datasets. Like the setting in Exp-I, we randomly chose four sets of examples from the testing set, each comprising `latex $8$ ` examples for GSM8K and `latex $6$ ` examples for StrategyQA. We then crafted hints and solutions featuring intermediate reasoning steps aided by GPT-4. These experiments were carried out on four LLMs: Llama2-7B, Llama2-13B, Llama2-70B, and Mixtral-8\*7B. According to the results presented in Tab. [5](#tab:robust), CoT+HSP consistently outperformed CoT across the GSM8K and StrategyQA datasets, with all four models showing significant performance enhancements across the four example sets. This demonstrates the robustness of the performance gains achieved by integrating CoT with HSP.

+--------------+--------------+-------------------------------------------+-------------------------------------------+
| Model | HSP | GSM8K | SQA |
+:=============+:============:+:========:+:========:+:========:+:========:+:========:+:========:+:========:+:========:+
| 3-6 (lr)7-10 | | E1 | E2 | E3 | E4 | E1 | E2 | E3 | E4 |
+--------------+--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| Lm2-7B | `latex $\times$ ` | 20.2 | 15.2 | 18.0 | 17.0 | 61.2 | 56.6 | 63.9 | 60.9 |
| +--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| | `latex $\checkmark$ ` | **22.7** | **21.6** | **23.4** | **22.8** | **63.8** | **61.5** | **65.9** | **63.3** |
+--------------+--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| Lm2-13B | `latex $\times$ ` | 35.9 | 29.1 | 25.4 | 32.2 | 64.1 | 60.6 | 67.5 | 63.2 |
| +--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| | `latex $\checkmark$ ` | **37.1** | **34.7** | **35.1** | **36.5** | **67.4** | **62.0** | **68.2** | **65.9** |
+--------------+--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| Lm2-70B | `latex $\times$ ` | 53.7 | 54.1 | 54.4 | 54.0 | 71.1 | 65.1 | 75.1 | 68.2 |
| +--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| | `latex $\checkmark$ ` | **60.1** | **56.3** | **55.3** | **59.3** | **71.7** | **72.1** | **75.8** | **73.1** |
+--------------+--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| Lm2-56B | `latex $\times$ ` | 67.9 | 68.8 | 67.2 | 67.8 | 65.4 | 60.3 | 69.3 | 61.9 |
| +--------------+----------+----------+----------+----------+----------+----------+----------+----------+
| | `latex $\checkmark$ ` | **69.1** | **69.1** | **68.2** | **68.8** | **67.3** | **64.5** | **70.6** | **66.8** |
+--------------+--------------+----------+----------+----------+----------+----------+----------+----------+----------+

: Experimental results for CoT Prompting with and without HSP on the GSM8K and StrategyQA (SQA) datasets across various example groups (E1, E2, E3, and E4). Values in bold denote the best results.

**Mathematical reasoning**

---

[Please answer the following question.]
[Example 1:] [Question]: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
[Hint]: Begin with the number of toys Shawn had initially. Then, add the number of toys he received from each parent. Remember, each parent gave him a certain number of toys, so you'll need to add those to his original amount to find out how many toys he has now.
[Solution]: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.
[\...\... (Omitting 7 examples)]
[Testing Example: ]
[Question]: \[QUESTION\]
**Commonsense reasoning**
[Please answer the following question.]
[Example 1:] [Question]: Do hamsters provide food for any animals?
[Hint]: Consider the natural role of hamsters in the food chain and who might rely on them as a source of nutrition.
[Solution]: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
[\...\... (Omitting 5 examples)]
[Testing Example: ]
[Question]: \[QUESTION\]

# Prompt Example

The four models evaluated in this paper, namely Lm2-7B, Lm2-13B, Lm2-70B, and Mix-56B, were all tested using the same prompt template. Fig. [\[tab:prompt_template\]](#tab:prompt_template) shows the prompt template for mathematical reasoning and common sense reasoning tasks.

# Case Study

Guiding the model to generate hints before the solution can effectively improve the model's performance. So, how does guiding LLM to generate hints first affect the generation of the model's solution? We choose to introduce hints under CoT prompting and select case studies on mathematical reasoning and common sense reasoning tasks, as shown in Tab  [\[tab:case_study_all\]](#tab:case_study_all).

#### Case 1

The solution from CoT appears logical, but its analysis remains superficial, merely focusing on the relationship between the two entities (US brand Nice and the Western honey bee) to answer the question. In contrast, the hint from CoT+HSP suggests approaching from a deeper view, specifically questioning whether the crops relied upon by US brand Nice depend on Western honey bees for pollination, which leads to the correct answer.

#### Case 2

For question "Do black-tailed jackrabbits fear the European wildcat?", CoT only considered the biological perspective, leading to an incorrect answer. However, the hint from CoT+HSP suggested that it is necessary to consider not only the biological aspect but also the habitat of the organism, thereby achieving the correct answer.

#### Case 3

We can observe that CoT's calculation method overlooks an important piece of knowledge, namely the formula for calculating the perimeter: "The distance traveled by a point on the edge of a rotating object equals the circle's circumference.". In contrast, CoT+Hint successfully suggests utilizing the formula for perimeter, thereby obtaining the correct answer.

#### Case 4

The question involves calculating the perimeter of a rectangle, but the CoT method only adds the width and height of a rectangle. CoT+HSP suggested that the perimeter be calculated by four lengths, making the final answer calculation correct.

**Case 1: Commonsense reasoning**

---

[Question]: Does US brand Nice depend on Western honey bee?
[True Answer]: TRUE
[CoT Response]: Nice is a brand of ice cream. Ice cream is not made from honey bees. Thus, Nice does not depend on Western honey bees. So the answer is no. [\[WRONG\]]
[CoT+HSP Response]:
[Hint: Consider the crops that Nice brand depends on and whether those crops require pollination by Western honey bees.]
Solution: Nice brand depends on almonds. Almonds require pollination by Western honey bees. Thus, Nice brand depends on Western honey bees. So the answer is yes. [\[CORRECT\]]
**Case 2: Commonsense reasoning**
[Question]: Do black-tailed jackrabbits fear the European wildcat?
[True Answer]: FALSE
[CoT Response]: Black-tailed jackrabbits are prey animals. European wildcats are predators. Thus, black-tailed jackrabbits would fear the European wildcat. So the answer is yes. [\[WRONG\]]
[CoT+HSP Response]:
[Hint: Consider the natural habitats of black-tailed jackrabbits and European wildcats and whether their paths would cross.]
Solution: Black-tailed jackrabbits are native to North America. European wildcats are native to Europe. Thus, their paths would not naturally cross. So the answer is no. [\[CORRECT\]]
**Case 3: Mathematical reasoning**
[Question]: A point on the edge of a fan blade that is rotating in a plane 10 centimeters from the center of the fan. What is the distance traveled, in centimeters, by this point after 30 seconds when the fan runs at the rate of 300 revolutions per minutes? \# Answer option: \[A)`latex $750\pi$ `, B)`latex $1500\pi$ `, C) `latex $1875\pi$ `, D)`latex $3000\pi$ `, E)`latex $7500\pi$ `\]
[True Answer]: D
[CoT Response]: The fan makes 300 revolutions in 60 seconds. So it makes 300/60 = 5 revolutions in 1 second. So it makes 5\*30 = 150 revolutions in 30 seconds. The point on the edge of the fan blade travels 10 centimeters in one revolution. So it travels 10\*150 = 1500 centimeters in 30 seconds. So the answer is (b). [\[WRONG\]]
[CoT+HSP Response]:
[Hint: The distance traveled by a point on the edge of a rotating object is calculated by multiplying the circumference of the circle it moves along by the number of rotations.]
Solution: The fan makes 300 revolutions in 60 seconds. In 30 seconds, it makes 300/2 = 150 revolutions. The circumference of the circle is `latex $2\pi r = 2\pi(10) = 20\pi$ `. The distance traveled is `latex $150 * 20\pi = 3000 \pi$ `. So the answer is (d). [\[CORRECT\]]
**Case 4: Mathematical reasoning**
[Question]: James wants to hang crepe-paper streamers from his backyard fence for his birthday party. His backyard fence is a rectangle that measures 20 feet on the long side and 15 feet on the short side. How many feet of crepe paper does James need to buy?
[True Answer]: 70
[CoT Response]: The fence is 20 feet long and 15 feet wide. The total length of the fence is 20 + 15 = 35 feet. He needs exactly the same amount of crepe paper as the length of the fence. So he needs to buy 35 feet of crepe paper. The answer is 35. [\[WRONG\]]
[CoT+HSP Response]:
[Hint: To find out how much crepe paper James needs to buy, calculate the perimeter of the fence. The perimeter is the total distance around the fence. To find the perimeter, add the lengths of all four sides of the fence together.]
Solution: The long side of the fence is 20 feet. The short side is 15 feet. So the perimeter is 20 + 15 + 20 + 15 = 70 feet. The answer is 70. [\[CORRECT\]]

# Reference Baseline

In this paper, we reimplemented the results of four models, namely Llama-7B, Llama-13B, Llama-70B, and Mixtral-7\*8B, under SD, LtM, PS, and CoT promptings, to compare with our HSP-enhanced promptings' performance. Are our reimplemented results within a reasonable range? To answer this question, we compared our reimplemented results with results from some recently works across six datasets: GSM8K, AQUA, ASDiv, Date, MultiArith, and StrategyQA. The results are shown in Fig. [6](#fig:reference_baseline).

There is a considerable amount of existing work on CoT prompting, while results for SD, LtM, and PS prompting are limit. The baseline work we present in the Fig. [6](#fig:reference_baseline) comes from five studies that cover a broad range of baseline methods. We can observe that across these six datasets, except for Llama-7B, which often lacks a closely matched model size for a baseline, the results for Llama-13B, Llama-70B, and Mixtral-7\*8B are comparable to some existing open-source or closed-source models.

# Results of Self-consistency

Tab. [\[tab:sc_full\]](#tab:sc_full) shows the results of self-consistency.

+:----------:+:-----------------------------------:+:--------------------------------------:+:----------------------------------------------:+:----------------------------------------------:+:----------------------------------------------:+:----------------------------------------------:+:----------------------------------------------:+:----------------------------------------------:+:-------------------------------------:+:------------------------------------------:+
| Model | SC | Hint | MATH | Commonsense | Avg | Relative Improvement |
| | | +------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+ | |
| | | | GSM8K | ASDiv | MultiArith | AQUA | SQA | Date | | |
+------------+-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+--------------------------------------------+
| Llama2-7B | 1 | 0 | 19.7 | 53.6 | 63.4 | 24.4 | 66.3 | [**40.1**] | 44.6 | [IMAGE: image] |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 1 | 1 | 19.9 | 55.8 | 63.8 | 24.4 | 67.5 | 43.2 | 45.8 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | [1] | [Impv] | [0.2] | [2.2] | [0.4] | [0.0] | [1.2] | [3.1] | [1.2] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 0 | 23.6 | 54.6 | 68.0 | 23.6 | 67.9 | [**40.1**] | 46.3 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 1 | 26.5 | 57.1 | 73.0 | 26.4 | 69.2 | 42.1 | 49.1 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | [Impv] | [2.9] | [2.5] | [5.0] | [2.8] | [1.3] | [2.0] | [2.8] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 0 | 24.7 | [**55.5**] | [**68.5**] | 26.0 | 67.9 | [**40.1**] | 47.1 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 1 | 29.2 | 57.3 | 77.7 | 26.0 | [**70.7**] | 43.5 | 50.7 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | [Impv] | [4.5] | [1.8] | [9.2] | [0.0] | [2.8] | [3.4] | [3.6] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 0 | [25.5] | 55.2 | 67.6 | 25.6 | [**68.6**] | 39.6 | 47.0 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 1 | 29.5 | 57.5 | 78.9 | [**26.4**] | 70.2 | 41.5 | 50.7 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | [Impv] | [4.0] | [2.3] | [11.3] | [0.8] | [1.6] | [1.9] | [3.6] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 0 | 25.4 | 55.1 | 68.1 | [**26.4**] | 68.3 | 40.4 | 47.3 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 1 | [**30.3**] | [**59.0**] | [**79.5**] | 25.6 | 70.2 | [**44.3**] | 51.5 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | [Impv] | [4.9] | [3.9] | [11.4] | [-0.8] | [1.9] | [3.9] | [4.2] | |
+------------+-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+--------------------------------------------+
| Llama2-13B | 1 | 0 | 34.5 | 60.5 | 83.2 | 25.6 | 68.0 | 52.4 | 54.0 | [IMAGE: image] |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 1 | 1 | 36.5 | 61.2 | 87.1 | 25.6 | 72.1 | 53.5 | 56.0 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 1 | [Impv] | [2.0] | [0.7] | [3.9] | [0.0] | [4.1] | [1.1] | [2.0] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 0 | 40.7 | 61.5 | 87.8 | 25.6 | 69.1 | 57.4 | 57.0 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 1 | 41.1 | 62.7 | 89.4 | 28.7 | 72.6 | 57.9 | 58.7 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | [Impv] | [0.4] | [1.2] | [1.6] | [3.1] | [3.5] | [0.5] | [1.7] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 0 | 42.3 | 62.5 | 89.4 | [**28.0**] | 69.0 | 57.7 | 58.2 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 1 | 46.9 | 64.7 | 91.3 | 28.7 | 72.8 | 59.1 | 60.6 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | [Impv] | [4.6] | [2.2] | [1.9] | [0.7] | [3.8] | [1.4] | [2.4] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 0 | 41.5 | 62.6 | 90.1 | 26.4 | 69.6 | 57.9 | 58.0 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 1 | 48.2 | 64.9 | 92.8 | 28.0 | [**73.1**] | 60.2 | 61.2 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | [Impv] | [6.7] | [2.3] | [2.7] | [1.6] | [3.5] | [2.3] | [3.2] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 0 | [**47.9**] | [**62.9**] | [**90.1**] | 27.6 | [**70.2**] | [**58.8**] | 59.6 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 1 | [**52.5**] | [**66.8**] | [**93.0**] | [**29.5**] | 73.0 | [**61.6**] | 62.7 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | [Impv] | [4.6] | [3.9] | [2.9] | [1.9] | [2.8] | [2.8] | [3.2] | |
+------------+-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+--------------------------------------------+
| Llama2-70B | 1 | 0 | 46.1 | 72.5 | 93.8 | 35.8 | 74.6 | 71.6 | 65.7 | [IMAGE: image] |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 1 | 1 | 50.3 | 74.4 | 94.6 | 37.0 | 77.0 | 73.0 | 67.7 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 1 | [Impv] | [4.2] | [1.9] | [0.8] | [1.2] | [2.4] | [1.4] | [2.0] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 0 | 59.5 | 75.0 | 95.3 | 39.8 | 78.2 | 73.3 | 70.2 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | 1 | 60.5 | 75.9 | 96.1 | 41.3 | 78.3 | 82.7 | 72.5 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 4 | [Impv] | [1.0] | [0.9] | [0.8] | [1.5] | [0.1] | [9.4] | [2.3] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 0 | 60.1 | 76.3 | 96.1 | 42.1 | 78.4 | 73.3 | 71.1 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | 1 | 67.0 | 77.9 | 97.8 | 44.5 | 79.2 | 74.9 | 73.6 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 16 | [Impv] | [6.9] | [1.6] | [1.7] | [2.4] | [0.8] | [1.6] | [2.5] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 0 | 60.6 | 77.1 | 96.3 | 45.3 | 78.5 | 72.7 | 71.8 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | 1 | 67.4 | 78.4 | 98.0 | [**47.2**] | [**80.1**] | [**84.1**] | 75.9 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 32 | [Impv] | [6.8] | [1.3] | [1.7] | [1.9] | [1.6] | [11.4] | [4.1] | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 0 | [**67.0**] | [**77.6**] | [**96.3**] | [**46.1**] | [**79.4**] | [**73.5**] | 73.3 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | 1 | [**67.6**] | [**78.8**] | [**98.2**] | 47.6 | 79.5 | 83.3 | 75.8 | |
| +-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+ |
| | 128 | [Impv] | [0.6] | [1.2] | [1.9] | [1.5] | [0.1] | [9.8] | [2.5] | |
+------------+-------------------------------------+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+---------------------------------------+--------------------------------------------+
